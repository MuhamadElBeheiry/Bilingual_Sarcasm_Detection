{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Arabic-Sarcasm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedahmedsaadahmed77/Sarcasm-Detection/blob/main/Arabic_Sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "aJhl9n0wnUu6",
        "outputId": "ea2b9c87-5e1d-4386-8468-0c1bc26a84d3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#/content/gdrive/MyDrive/Colab Notebooks/selected3/ArSarcasm_train.csv\n",
        "!git clone https://github.com/mohamedahmedsaadahmed77/Sarcasm_Detection.git\n",
        "data = pd.read_csv(\"/content/Sarcasm_Detection/ArSarcasm_train.csv\")\n",
        "data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Cloning into 'Sarcasm_Detection'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 16 (delta 7), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialect</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>original_sentiment</th>\n",
              "      <th>tweet</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gulf</td>\n",
              "      <td>False</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>\"نصيحه ما عمرك اتنزل لعبة سوبر ماريو مش زي ما ...</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>msa</td>\n",
              "      <td>False</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>\"#نادين_نسيب_نجيم ❤️❤️❤️مجلة #ماري_كلير 💭#ملكة...</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>egypt</td>\n",
              "      <td>False</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>\"@Alito_NBA اتوقع انه بيستمر\"</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>levant</td>\n",
              "      <td>True</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>\"@KSA24 يعني \"بموافقتنا\" لأن دمشق صايرة موسكو\"</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>msa</td>\n",
              "      <td>False</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>\"RT @alaahmad20: قائد في الحرس يعترف بفقدان ال...</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8432</th>\n",
              "      <td>msa</td>\n",
              "      <td>False</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>\"RT @iDhmi: #توصيات_تويتسو | تطبيق يوتيوب الأط...</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8433</th>\n",
              "      <td>msa</td>\n",
              "      <td>False</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>سي بي سي:#إبراهيم_محلب يشارك باحتفالية تنزانيا...</td>\n",
              "      <td>astd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8434</th>\n",
              "      <td>msa</td>\n",
              "      <td>True</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>\"كلب إسرائيل وسيفها الإرهابي بوتن قاتل النساء ...</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8435</th>\n",
              "      <td>msa</td>\n",
              "      <td>True</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>\"احمد فتفت صار مغضوب عليه  بمجرد اعترضو على مي...</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8436</th>\n",
              "      <td>msa</td>\n",
              "      <td>False</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>\"قناة آسيا: مندوب #سوريا في #الأمم_المتحدة بشا...</td>\n",
              "      <td>semeval</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8437 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     dialect  ...   source\n",
              "0       gulf  ...  semeval\n",
              "1        msa  ...  semeval\n",
              "2      egypt  ...  semeval\n",
              "3     levant  ...  semeval\n",
              "4        msa  ...  semeval\n",
              "...      ...  ...      ...\n",
              "8432     msa  ...  semeval\n",
              "8433     msa  ...     astd\n",
              "8434     msa  ...  semeval\n",
              "8435     msa  ...  semeval\n",
              "8436     msa  ...  semeval\n",
              "\n",
              "[8437 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCixlAyKnUu9"
      },
      "source": [
        "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
        "\n",
        "# Arabic stop words with nltk\n",
        "stop_words = stopwords.words()\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Shadda\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQh9-THRWzm0"
      },
      "source": [
        "def preprocess(text):\r\n",
        "    \r\n",
        "    '''\r\n",
        "    text is an arabic string input\r\n",
        "    \r\n",
        "    the preprocessed text is returned\r\n",
        "    '''\r\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\r\n",
        "\r\n",
        "\r\n",
        "    #remove punctuations\r\n",
        "\r\n",
        "    translator = str.maketrans('', '', punctuations)\r\n",
        "    text = text.translate(translator)\r\n",
        "    \r\n",
        "    # remove Tashkeel\r\n",
        "    text = re.sub(arabic_diacritics, '', text)\r\n",
        "    \r\n",
        "    #remove longation\r\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\r\n",
        "    text = re.sub(\"ى\", \"ي\", text)\r\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\r\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\r\n",
        "    text = re.sub(\"ة\", \"ه\", text)\r\n",
        "    text = re.sub(\"گ\", \"ك\", text)\r\n",
        "\r\n",
        "    text = [word for word in text.split() if word not in stop_words]\r\n",
        "\r\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb6t_tpUtYES"
      },
      "source": [
        "feature = [preprocess(x) for x in data.tweet]\r\n",
        "target = data.sarcasm\r\n",
        "# splitting into train and tests\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3zFyZjknUvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c595fc-d6c3-4cc6-ddde-6f0826c47b28"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import re\r\n",
        "import string\r\n",
        "import csv\r\n",
        "\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "class TextNBClassifier:\r\n",
        "    def __init__(self):\r\n",
        "        pass\r\n",
        "    \r\n",
        "    def count_words_freq(self, xs, ys):\r\n",
        "      freqs = {}\r\n",
        "      for words, y in zip(xs, ys):\r\n",
        "          for word in words:\r\n",
        "              pair = (word,y)\r\n",
        "              if pair in freqs:\r\n",
        "                  freqs[pair] += 1\r\n",
        "              else:\r\n",
        "                  freqs[pair] = 1\r\n",
        "      return freqs\r\n",
        "\r\n",
        "    def lookup(self, freqs, word, is_sarcastic):\r\n",
        "      if (word, is_sarcastic) in freqs:\r\n",
        "        return freqs[(word, is_sarcastic)]\r\n",
        "      else:\r\n",
        "        return 0\r\n",
        "      return freq\r\n",
        "\r\n",
        "    def fit(self, train_x, train_y):\r\n",
        "      freqs = self.count_words_freq(train_x, train_y)\r\n",
        "      self.loglikelihood = {}\r\n",
        "      self.logprior = 0\r\n",
        "\r\n",
        "      vocab = set([pair[0] for pair in freqs.keys()])\r\n",
        "      V = len(vocab)\r\n",
        "\r\n",
        "      N_pos = N_neg = V_pos = V_neg = 0\r\n",
        "      for pair in freqs.keys():\r\n",
        "          if pair[1] > 0:\r\n",
        "              V_pos += 1\r\n",
        "              N_pos += freqs[pair]\r\n",
        "          else:\r\n",
        "              V_neg += 1\r\n",
        "              N_neg += freqs[pair]\r\n",
        "      D = len(train_y)\r\n",
        "      \r\n",
        "      D_pos = (len(list(filter(lambda x: x > 0, train_y))))\r\n",
        "      D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\r\n",
        "\r\n",
        "      logprior = np.log(D_pos) - np.log(D_neg)\r\n",
        "\r\n",
        "      for word in vocab:\r\n",
        "          freq_pos = self.lookup(freqs,word,1)\r\n",
        "          freq_neg = self.lookup(freqs,word,0)\r\n",
        "\r\n",
        "          p_w_pos = (freq_pos + 1) / (N_pos + V)\r\n",
        "          p_w_neg = (freq_neg + 1) / (N_neg + V)\r\n",
        "\r\n",
        "          self.loglikelihood[word] = np.log(p_w_pos/p_w_neg)\r\n",
        "      return self\r\n",
        "\r\n",
        "    def predict_per_record(self, word_l):\r\n",
        "      p = 0\r\n",
        "      p += self.logprior\r\n",
        "\r\n",
        "      for word in word_l:\r\n",
        "        if word in self.loglikelihood:\r\n",
        "              p += self.loglikelihood[word]\r\n",
        "      return p\r\n",
        "\r\n",
        "    def predict(self, test_x, test_y):\r\n",
        "      self.accuracy = 0\r\n",
        "      y_hats = []\r\n",
        "      for word_l in test_x:\r\n",
        "          if self.predict_per_record(word_l) > 0:\r\n",
        "              y_hat_i = 1\r\n",
        "          else:\r\n",
        "              y_hat_i = 0\r\n",
        "          y_hats.append(y_hat_i)\r\n",
        "      error = np.mean(np.absolute(y_hats-test_y))\r\n",
        "      self.accuracy = 1-error    \r\n",
        "      return self"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHidf5RdIsV_",
        "outputId": "264adc52-161d-452c-9f0d-c7fccaad0364"
      },
      "source": [
        "nbc = TextNBClassifier()\r\n",
        "nbc.fit(X_train,Y_train)\r\n",
        "nbc.predict(X_train, Y_train)\r\n",
        "print(\"Training Accuracy : %.2f\"%(nbc.accuracy * 100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 96.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jlydP-xhe4B",
        "outputId": "8648ba65-290b-4f79-e1e1-c08920b921aa"
      },
      "source": [
        "nbc.predict(X_test, Y_test)\r\n",
        "print(\"Testing Accuracy : %.2f\"%(nbc.accuracy * 100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy : 83.06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}