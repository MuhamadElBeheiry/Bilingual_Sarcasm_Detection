{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English-Sarcasm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmDyok3y1+0xdx7jpxjbLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedahmedsaadahmed77/Sarcasm-Detection/blob/main/English_Sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8M2oMT-jrgQ",
        "outputId": "6784a7c9-2b3e-4eb8-e5c8-1e73b2f263f9"
      },
      "source": [
        "!git clone https://github.com/mohamedahmedsaadahmed77/Sarcasm_Detection.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Sarcasm_Detection'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 16 (delta 7), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aveZYQwPAXgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b11c3c-b0bf-41b6-c15f-9e74274160be"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import re\r\n",
        "import string\r\n",
        "import csv\r\n",
        "\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0vjEO8uAXZN"
      },
      "source": [
        "def preprocessing(sentences):\r\n",
        "  lowercase = lambda x : x.lower()\r\n",
        "  sentences = list(map(lowercase, sentences))\r\n",
        "\r\n",
        "  remove_punctuation = lambda x : re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\r\n",
        "  sentences = list(map(remove_punctuation, sentences))\r\n",
        "  \r\n",
        "  remove_digits = lambda x : re.sub('\\w*\\d\\w*', ' ', x)\r\n",
        "  sentences = list(map(remove_digits, sentences))\r\n",
        "  \r\n",
        "  convert_to_tokens = lambda x : word_tokenize(x)\r\n",
        "  tokens = list(map(convert_to_tokens, sentences))\r\n",
        "\r\n",
        "  stemmer = PorterStemmer()\r\n",
        "  stem_tokens = lambda x : [stemmer.stem(w) for w in x]\r\n",
        "  stemmed_tokens = list(map(stem_tokens, tokens))\r\n",
        "  \r\n",
        "  stop_words = set(stopwords.words('english'))\r\n",
        "  remove_stopwords = lambda x : [w for w in x if not w in stop_words]\r\n",
        "  stemmed_tokens = list(map(remove_stopwords, stemmed_tokens))\r\n",
        "  \r\n",
        "  return stemmed_tokens"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQZdavvcYOm7"
      },
      "source": [
        "class TextNBClassifier:\r\n",
        "    def __init__(self):\r\n",
        "        pass\r\n",
        "    \r\n",
        "    def count_words_freq(self, xs, ys):\r\n",
        "      freqs = {}\r\n",
        "      for words, y in zip(xs, ys):\r\n",
        "          for word in words:\r\n",
        "              pair = (word,y)\r\n",
        "              if pair in freqs:\r\n",
        "                  freqs[pair] += 1\r\n",
        "              else:\r\n",
        "                  freqs[pair] = 1\r\n",
        "      return freqs\r\n",
        "\r\n",
        "    def lookup(self, freqs, word, is_sarcastic):\r\n",
        "      if (word, is_sarcastic) in freqs:\r\n",
        "        return freqs[(word, is_sarcastic)]\r\n",
        "      else:\r\n",
        "        return 0\r\n",
        "      return freq\r\n",
        "\r\n",
        "    def fit(self, train_x, train_y):\r\n",
        "      freqs = self.count_words_freq(train_x, train_y)\r\n",
        "      self.loglikelihood = {}\r\n",
        "      self.logprior = 0\r\n",
        "\r\n",
        "      vocab = set([pair[0] for pair in freqs.keys()])\r\n",
        "      V = len(vocab)\r\n",
        "\r\n",
        "      N_pos = N_neg = V_pos = V_neg = 0\r\n",
        "      for pair in freqs.keys():\r\n",
        "          if pair[1] > 0:\r\n",
        "              V_pos += 1\r\n",
        "              N_pos += freqs[pair]\r\n",
        "          else:\r\n",
        "              V_neg += 1\r\n",
        "              N_neg += freqs[pair]\r\n",
        "      D = len(train_y)\r\n",
        "      \r\n",
        "      D_pos = (len(list(filter(lambda x: x > 0, train_y))))\r\n",
        "      D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\r\n",
        "\r\n",
        "      logprior = np.log(D_pos) - np.log(D_neg)\r\n",
        "\r\n",
        "      for word in vocab:\r\n",
        "          freq_pos = self.lookup(freqs,word,1)\r\n",
        "          freq_neg = self.lookup(freqs,word,0)\r\n",
        "\r\n",
        "          p_w_pos = (freq_pos + 1) / (N_pos + V)\r\n",
        "          p_w_neg = (freq_neg + 1) / (N_neg + V)\r\n",
        "\r\n",
        "          self.loglikelihood[word] = np.log(p_w_pos/p_w_neg)\r\n",
        "      return self\r\n",
        "\r\n",
        "    def predict_per_record(self, word_l):\r\n",
        "      p = 0\r\n",
        "      p += self.logprior\r\n",
        "\r\n",
        "      for word in word_l:\r\n",
        "        if word in self.loglikelihood:\r\n",
        "              p += self.loglikelihood[word]\r\n",
        "      return p\r\n",
        "\r\n",
        "    def predict(self, test_x, test_y):\r\n",
        "      self.accuracy = 0\r\n",
        "      y_hats = []\r\n",
        "      for word_l in test_x:\r\n",
        "          if self.predict_per_record(word_l) > 0:\r\n",
        "              y_hat_i = 1\r\n",
        "          else:\r\n",
        "              y_hat_i = 0\r\n",
        "          y_hats.append(y_hat_i)\r\n",
        "      error = np.mean(np.absolute(y_hats-test_y))\r\n",
        "      self.accuracy = 1-error    \r\n",
        "      return self"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dITiRITaAXde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fdde0d93-0173-4a3b-9cec-886f761ee257"
      },
      "source": [
        "data = pd.read_csv ('/content/Sarcasm_Detection/English_Sarcasm.csv')\r\n",
        "print(data.shape)\r\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28619, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0             1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3             1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0rz0zvNAXa7"
      },
      "source": [
        "x_train, x_test, Y_train, Y_test = train_test_split(preprocessing(data['headline']), data['is_sarcastic'], test_size=0.2, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkQylqGRfeIr",
        "outputId": "e795269c-f95b-4f02-a856-c64eff0d5368"
      },
      "source": [
        "model = TextNBClassifier()\r\n",
        "model.fit(x_train, Y_train)\r\n",
        "model.predict(x_train, Y_train)\r\n",
        "print(\"Training Accuracy : %.2f\"%(model.accuracy * 100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 87.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "madQY4qYhLlK",
        "outputId": "f46a4c00-a5e5-4589-e4e2-f292481bb1f2"
      },
      "source": [
        "model.predict(x_test, Y_test)\r\n",
        "print(\"Test Accuracy : %.2f\"%(model.accuracy * 100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy : 79.89\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}